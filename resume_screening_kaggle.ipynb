{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Screening & Ranking System\n",
    "## Automated Resume Processing from Kaggle Datasets\n",
    "\n",
    "This notebook automatically loads and processes resumes from:\n",
    "- **Resume Dataset** (CSV format)\n",
    "- **Job Description Dataset** (CSV format)\n",
    "- **PDF/TXT files** (optional)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyPDF2 available for PDF processing\n",
      "All core libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Scikit-learn for ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PDF processing\n",
    "try:\n",
    "    import PyPDF2\n",
    "    PDF_AVAILABLE = True\n",
    "    print(\"PyPDF2 available for PDF processing\")\n",
    "except ImportError:\n",
    "    PDF_AVAILABLE = False\n",
    "    print(\"PyPDF2 not installed. Install with: pip install PyPDF2\")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All core libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NLTK Data and Load spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model loaded successfully\n",
      "Loaded 198 stopwords\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "# Load spaCy model\n",
    "try:\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    print(\"spaCy model loaded successfully\")\n",
    "except:\n",
    "    print(\"spaCy model not found. Run: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "# Get stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f\"Loaded {len(stop_words)} stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Place the CSV files in a `data/` folder or update the paths below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "  resume_csv: D:\\Padhai\\git\\FUTURE_ML_03\\Resume.csv\n",
      "  job_csv: D:\\Padhai\\git\\FUTURE_ML_03\\monster_com-job_sample.csv\n",
      "  resume_pdf_dir: data/resumes_pdf/\n",
      "  resume_txt_dir: data/resumes_txt/\n",
      "  text_similarity_weight: 0.3\n",
      "  skill_match_weight: 0.7\n",
      "  top_n_candidates: 10\n",
      "  test_size: 0.2\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    # Dataset paths (update these based on your setup)\n",
    "    'resume_csv': 'D:\\Padhai\\git\\FUTURE_ML_03\\Resume.csv',  # Resume dataset from Kaggle\n",
    "    'job_csv': 'D:\\Padhai\\git\\FUTURE_ML_03\\monster_com-job_sample.csv',  # Job description dataset\n",
    "    \n",
    "    # Alternative: Use PDF/TXT files from directories\n",
    "    'resume_pdf_dir': 'data/resumes_pdf/',\n",
    "    'resume_txt_dir': 'data/resumes_txt/',\n",
    "    \n",
    "    # Scoring weights\n",
    "    'text_similarity_weight': 0.3,\n",
    "    'skill_match_weight': 0.7,\n",
    "    \n",
    "    # How many candidates to show in detail\n",
    "    'top_n_candidates': 10,\n",
    "    \n",
    "    # Train/test split ratio\n",
    "    'test_size': 0.2,  # 20% for testing, 80% for training\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Comprehensive Skill Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skill database: 167 unique skills across 11 categories\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive skill database\n",
    "SKILL_DATABASE = {\n",
    "    'programming_languages': [\n",
    "        'python', 'java', 'javascript', 'c++', 'c#', 'ruby', 'php', 'swift',\n",
    "        'kotlin', 'go', 'rust', 'typescript', 'scala', 'r', 'matlab', 'sql',\n",
    "        'html', 'css', 'bash', 'perl', 'c', 'objective-c', 'vba'\n",
    "    ],\n",
    "    'ml_frameworks': [\n",
    "        'tensorflow', 'pytorch', 'keras', 'scikit-learn', 'sklearn', 'xgboost',\n",
    "        'lightgbm', 'catboost', 'hugging face', 'transformers', 'opencv'\n",
    "    ],\n",
    "    'data_science': [\n",
    "        'pandas', 'numpy', 'scipy', 'matplotlib', 'seaborn', 'plotly',\n",
    "        'data analysis', 'data visualization', 'statistics', 'machine learning',\n",
    "        'deep learning', 'neural networks', 'nlp', 'computer vision',\n",
    "        'natural language processing', 'feature engineering', 'model deployment',\n",
    "        'data mining', 'predictive modeling', 'statistical analysis'\n",
    "    ],\n",
    "    'web_technologies': [\n",
    "        'react', 'angular', 'vue', 'vue.js', 'node.js', 'nodejs', 'django', 'flask',\n",
    "        'spring', 'express', 'fastapi', 'rest', 'api', 'graphql', 'jquery',\n",
    "        'bootstrap', 'tailwind', 'asp.net', 'laravel', 'rails', 'webpack'\n",
    "    ],\n",
    "    'databases': [\n",
    "        'mysql', 'postgresql', 'mongodb', 'redis', 'cassandra', 'oracle',\n",
    "        'sql server', 'dynamodb', 'elasticsearch', 'sqlite', 'nosql',\n",
    "        'mariadb', 'neo4j', 'couchdb'\n",
    "    ],\n",
    "    'cloud_devops': [\n",
    "        'aws', 'azure', 'gcp', 'google cloud', 'docker', 'kubernetes',\n",
    "        'jenkins', 'gitlab', 'terraform', 'ansible', 'ci/cd', 'devops',\n",
    "        'linux', 'unix', 'git', 'github', 'nginx', 'apache', 'heroku'\n",
    "    ],\n",
    "    'big_data': [\n",
    "        'hadoop', 'spark', 'kafka', 'airflow', 'hive', 'etl', 'data pipeline',\n",
    "        'big data', 'mapreduce', 'pig', 'flink', 'storm'\n",
    "    ],\n",
    "    'business_tools': [\n",
    "        'excel', 'powerpoint', 'word', 'tableau', 'power bi', 'sap', 'salesforce',\n",
    "        'jira', 'confluence', 'microsoft office', 'google analytics', 'crm', 'erp'\n",
    "    ],\n",
    "    'soft_skills': [\n",
    "        'leadership', 'communication', 'teamwork', 'problem solving',\n",
    "        'analytical', 'critical thinking', 'presentation', 'collaboration',\n",
    "        'agile', 'scrum', 'project management', 'time management',\n",
    "        'creative', 'innovative', 'strategic thinking'\n",
    "    ],\n",
    "    'design': [\n",
    "        'figma', 'sketch', 'photoshop', 'illustrator', 'ui', 'ux',\n",
    "        'user experience', 'user interface', 'wireframing', 'prototyping'\n",
    "    ],\n",
    "    'testing': [\n",
    "        'selenium', 'junit', 'pytest', 'testing', 'qa', 'quality assurance',\n",
    "        'test automation', 'unit testing', 'integration testing'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Flatten to single set\n",
    "ALL_SKILLS = set()\n",
    "for category, skills in SKILL_DATABASE.items():\n",
    "    ALL_SKILLS.update(skills)\n",
    "\n",
    "print(f\"Skill database: {len(ALL_SKILLS)} unique skills across {len(SKILL_DATABASE)} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File processing functions defined\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "    \"\"\"\n",
    "    if not PDF_AVAILABLE:\n",
    "        print(\"PyPDF2 not available. Install with: pip install PyPDF2\")\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        text = \"\"\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_text_from_txt(txt_path):\n",
    "    \"\"\"\n",
    "    Extract text from a TXT file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {txt_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def load_resumes_from_directory(directory, file_type='txt'):\n",
    "    \"\"\"\n",
    "    Load all resumes from a directory.\n",
    "    \n",
    "    Args:\n",
    "        directory: Path to directory containing resume files\n",
    "        file_type: 'txt' or 'pdf'\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with filename: text content\n",
    "    \"\"\"\n",
    "    resumes = {}\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory not found: {directory}\")\n",
    "        return resumes\n",
    "    \n",
    "    if file_type == 'pdf':\n",
    "        pattern = os.path.join(directory, '*.pdf')\n",
    "        extract_func = extract_text_from_pdf\n",
    "    else:\n",
    "        pattern = os.path.join(directory, '*.txt')\n",
    "        extract_func = extract_text_from_txt\n",
    "    \n",
    "    files = glob.glob(pattern)\n",
    "    print(f\"Found {len(files)} {file_type.upper()} files in {directory}\")\n",
    "    \n",
    "    for filepath in files:\n",
    "        filename = os.path.basename(filepath)\n",
    "        candidate_name = os.path.splitext(filename)[0]\n",
    "        text = extract_func(filepath)\n",
    "        \n",
    "        if text.strip():\n",
    "            resumes[candidate_name] = text\n",
    "    \n",
    "    return resumes\n",
    "\n",
    "\n",
    "print(\"File processing functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Resume Dataset from Kaggle CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded resume dataset with 2484 resumes\n",
      "  Columns: ['ID', 'Resume_str', 'Resume_html', 'Category']\n",
      "  Using column 'Resume_str' for resume text\n",
      "  Found 24 unique job categories\n",
      "  Categories: <StringArray>\n",
      "[                    'HR',               'DESIGNER', 'INFORMATION-TECHNOLOGY',\n",
      "                'TEACHER',               'ADVOCATE',   'BUSINESS-DEVELOPMENT',\n",
      "             'HEALTHCARE',                'FITNESS',            'AGRICULTURE',\n",
      "                    'BPO']\n",
      "Length: 10, dtype: str\n"
     ]
    }
   ],
   "source": [
    "def load_resume_dataset(csv_path):\n",
    "    \"\"\"\n",
    "    Load resume dataset from Kaggle CSV.\n",
    "    Expected columns: 'Resume' or 'Resume_str', 'Category'\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Resume CSV not found at: {csv_path}\")\n",
    "        print(\"   Download from: https://www.kaggle.com/datasets/snehaanbhawal/resume-dataset\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded resume dataset with {len(df)} resumes\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Check for resume text column (different datasets use different names)\n",
    "        resume_col = None\n",
    "        for col in ['Resume', 'Resume_str', 'resume', 'resume_text', 'text']:\n",
    "            if col in df.columns:\n",
    "                resume_col = col\n",
    "                break\n",
    "        \n",
    "        if resume_col is None:\n",
    "            print(\"Could not find resume text column. Using first text column.\")\n",
    "            resume_col = df.columns[0]\n",
    "        \n",
    "        print(f\"  Using column '{resume_col}' for resume text\")\n",
    "        \n",
    "        # Check for category column\n",
    "        category_col = None\n",
    "        for col in ['Category', 'category', 'job_category', 'role']:\n",
    "            if col in df.columns:\n",
    "                category_col = col\n",
    "                break\n",
    "        \n",
    "        if category_col:\n",
    "            print(f\"  Found {df[category_col].nunique()} unique job categories\")\n",
    "            print(f\"  Categories: {df[category_col].unique()[:10]}\")\n",
    "        \n",
    "        return df, resume_col, category_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading resume dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "resume_data = load_resume_dataset(CONFIG['resume_csv'])\n",
    "\n",
    "if resume_data:\n",
    "    resume_df, resume_text_col, resume_category_col = resume_data\n",
    "else:\n",
    "    resume_df = None\n",
    "    print(\"\\nWill try to load from PDF/TXT directories...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Job Description Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded job dataset with 22000 job postings\n",
      "  Columns: ['country', 'country_code', 'date_added', 'has_expired', 'job_board', 'job_description', 'job_title', 'job_type', 'location', 'organization', 'page_url', 'salary', 'sector', 'uniq_id']\n",
      "  Using 'job_title' for job titles\n",
      "  Using 'job_description' for job descriptions\n"
     ]
    }
   ],
   "source": [
    "def load_job_dataset(csv_path):\n",
    "    \"\"\"\n",
    "    Load job description dataset from Kaggle CSV.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Job description CSV not found at: {csv_path}\")\n",
    "        print(\"   Download from: https://www.kaggle.com/datasets/PromptCloudHQ/us-jobs-on-monstercom\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded job dataset with {len(df)} job postings\")\n",
    "        print(f\"  Columns: {list(df.columns)}\")\n",
    "        \n",
    "        # Find relevant columns\n",
    "        title_col = None\n",
    "        desc_col = None\n",
    "        \n",
    "        for col in ['job_title', 'title', 'position', 'role']:\n",
    "            if col in df.columns:\n",
    "                title_col = col\n",
    "                break\n",
    "        \n",
    "        for col in ['job_description', 'description', 'desc', 'job_details']:\n",
    "            if col in df.columns:\n",
    "                desc_col = col\n",
    "                break\n",
    "        \n",
    "        print(f\"  Using '{title_col}' for job titles\")\n",
    "        print(f\"  Using '{desc_col}' for job descriptions\")\n",
    "        \n",
    "        return df, title_col, desc_col\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading job dataset: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Load job dataset\n",
    "job_data = load_job_dataset(CONFIG['job_csv'])\n",
    "\n",
    "if job_data:\n",
    "    job_df, job_title_col, job_desc_col = job_data\n",
    "else:\n",
    "    job_df = None\n",
    "    print(\"\\nNo job dataset loaded. Will use sample job description.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative - Load from PDF/TXT Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If CSV not available, try loading from directories\n",
    "if resume_df is None:\n",
    "    print(\"Attempting to load resumes from PDF/TXT directories...\")\n",
    "    \n",
    "    # Try PDF directory\n",
    "    pdf_resumes = load_resumes_from_directory(CONFIG['resume_pdf_dir'], 'pdf')\n",
    "    \n",
    "    # Try TXT directory\n",
    "    txt_resumes = load_resumes_from_directory(CONFIG['resume_txt_dir'], 'txt')\n",
    "    \n",
    "    # Combine\n",
    "    all_resumes = {**pdf_resumes, **txt_resumes}\n",
    "    \n",
    "    if all_resumes:\n",
    "        # Convert to DataFrame\n",
    "        resume_df = pd.DataFrame([\n",
    "            {'candidate_name': name, 'resume_text': text, 'category': 'Unknown'}\n",
    "            for name, text in all_resumes.items()\n",
    "        ])\n",
    "        resume_text_col = 'resume_text'\n",
    "        resume_category_col = 'category'\n",
    "        print(f\"Created DataFrame with {len(resume_df)} resumes from files\")\n",
    "    else:\n",
    "        print(\"\\nNo resume data found!\")\n",
    "        print(\"   Please either:\")\n",
    "        print(\"   1. Download Kaggle resume dataset\")\n",
    "        print(\"   2. Place PDF/TXT resumes in data directories\")\n",
    "        print(\"   3. Update CONFIG paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing functions ready\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    \n",
    "    # Remove emails\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove phone numbers\n",
    "    text = re.sub(r'\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b', '', text)\n",
    "    \n",
    "    # Remove special characters but keep important ones for skills\n",
    "    text = re.sub(r'[^a-z0-9\\s\\+\\#\\.]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_skills(text, skill_set=ALL_SKILLS):\n",
    "    \"\"\"\n",
    "    Extract skills from text using pattern matching.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return set()\n",
    "    \n",
    "    found_skills = set()\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    # Direct matching\n",
    "    for skill in skill_set:\n",
    "        pattern = r'\\b' + re.escape(skill) + r'\\b'\n",
    "        if re.search(pattern, text_lower):\n",
    "            found_skills.add(skill)\n",
    "    \n",
    "    # Handle common variations\n",
    "    if 'sklearn' in text_lower or 'scikit' in text_lower:\n",
    "        found_skills.add('scikit-learn')\n",
    "    if 'node.js' in text_lower or 'nodejs' in text_lower:\n",
    "        found_skills.add('node.js')\n",
    "    if 'ml' in text_lower.split():\n",
    "        found_skills.add('machine learning')\n",
    "    \n",
    "    return found_skills\n",
    "\n",
    "\n",
    "print(\"Text preprocessing functions ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Resume Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2484 resumes...\n",
      "  → Cleaning text...\n",
      "  → Extracting skills...\n",
      "\n",
      "Processed 2483 resumes\n",
      "  Average skills per resume: 5.3\n",
      "  Min skills: 0\n",
      "  Max skills: 41\n",
      "\n",
      "Sample processed resume:\n",
      "  Candidate: Candidate_1\n",
      "  Category: HR\n",
      "  Skills found: 6\n",
      "  Skills: ['statistics', 'data analysis', 'time management', 'leadership', 'swift', 'analytical']...\n"
     ]
    }
   ],
   "source": [
    "if resume_df is not None:\n",
    "    print(f\"Processing {len(resume_df)} resumes...\")\n",
    "    \n",
    "    # Add candidate ID if not present\n",
    "    if 'candidate_name' not in resume_df.columns:\n",
    "        resume_df['candidate_name'] = [f\"Candidate_{i+1}\" for i in range(len(resume_df))]\n",
    "    \n",
    "    # Clean text\n",
    "    print(\"  → Cleaning text...\")\n",
    "    resume_df['cleaned_text'] = resume_df[resume_text_col].apply(clean_text)\n",
    "    \n",
    "    # Extract skills\n",
    "    print(\"  → Extracting skills...\")\n",
    "    resume_df['extracted_skills'] = resume_df[resume_text_col].apply(extract_skills)\n",
    "    resume_df['num_skills'] = resume_df['extracted_skills'].apply(len)\n",
    "    \n",
    "    # Remove empty resumes\n",
    "    resume_df = resume_df[resume_df['cleaned_text'].str.len() > 50].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\nProcessed {len(resume_df)} resumes\")\n",
    "    print(f\"  Average skills per resume: {resume_df['num_skills'].mean():.1f}\")\n",
    "    print(f\"  Min skills: {resume_df['num_skills'].min()}\")\n",
    "    print(f\"  Max skills: {resume_df['num_skills'].max()}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample processed resume:\")\n",
    "    sample_idx = 0\n",
    "    print(f\"  Candidate: {resume_df.iloc[sample_idx]['candidate_name']}\")\n",
    "    if resume_category_col:\n",
    "        print(f\"  Category: {resume_df.iloc[sample_idx][resume_category_col]}\")\n",
    "    print(f\"  Skills found: {resume_df.iloc[sample_idx]['num_skills']}\")\n",
    "    print(f\"  Skills: {list(resume_df.iloc[sample_idx]['extracted_skills'])[:10]}...\")\n",
    "else:\n",
    "    print(\"No resume data to process!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select or Create Job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected job from dataset: Johnson & Johnson Family of Companies Job Application for Senior Training Leader | Monster.com var MONS_LOG_VARS = {\"JobID\":\n",
      "\n",
      "Job: Johnson & Johnson Family of Companies Job Application for Senior Training Leader | Monster.com var MONS_LOG_VARS = {\"JobID\":\n",
      "Required skills found: 11\n",
      "Skills: ['agile', 'collaboration', 'communication', 'excel', 'innovative', 'leadership', 'microsoft office', 'powerpoint', 'project management', 'r', 'word']\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Use job from dataset\n",
    "if job_df is not None and len(job_df) > 0:\n",
    "    # Filter for ML/Data Science related jobs\n",
    "    ml_keywords = ['machine learning', 'data scientist', 'ml engineer', 'ai', 'data science']\n",
    "    \n",
    "    job_df['is_ml_job'] = job_df[job_title_col].str.lower().apply(\n",
    "        lambda x: any(keyword in str(x).lower() for keyword in ml_keywords) if pd.notna(x) else False\n",
    "    )\n",
    "    \n",
    "    ml_jobs = job_df[job_df['is_ml_job']]\n",
    "    \n",
    "    if len(ml_jobs) > 0:\n",
    "        # Select first ML job\n",
    "        selected_job = ml_jobs.iloc[0]\n",
    "        job_title = selected_job[job_title_col]\n",
    "        job_description = selected_job[job_desc_col]\n",
    "        print(f\"Selected job from dataset: {job_title}\")\n",
    "    else:\n",
    "        # Select any job\n",
    "        selected_job = job_df.iloc[0]\n",
    "        job_title = selected_job[job_title_col]\n",
    "        job_description = selected_job[job_desc_col]\n",
    "        print(f\"Selected job from dataset: {job_title}\")\n",
    "else:\n",
    "    # Option 2: Use sample job description\n",
    "    print(\"Using sample job description...\")\n",
    "    job_title = \"Senior Machine Learning Engineer\"\n",
    "    job_description = \"\"\"\n",
    "    SENIOR MACHINE LEARNING ENGINEER\n",
    "    \n",
    "    We are seeking an experienced Senior Machine Learning Engineer to join our AI team.\n",
    "    \n",
    "    RESPONSIBILITIES:\n",
    "    - Design and implement ML models using TensorFlow and PyTorch\n",
    "    - Build deep learning architectures for various applications\n",
    "    - Develop MLOps pipelines for model deployment\n",
    "    - Work with large datasets using Pandas and NumPy\n",
    "    - Deploy models on AWS using Docker and Kubernetes\n",
    "    - Collaborate with cross-functional teams\n",
    "    - Conduct experiments and statistical analysis\n",
    "    - Mentor junior team members\n",
    "    \n",
    "    REQUIRED:\n",
    "    - 5+ years experience in machine learning or data science\n",
    "    - Strong Python programming skills\n",
    "    - Expert in TensorFlow, PyTorch, or Keras\n",
    "    - Experience with deep learning and neural networks\n",
    "    - Proficiency in Pandas, NumPy, scikit-learn\n",
    "    - Experience deploying ML models to production\n",
    "    - Knowledge of AWS, Docker, Kubernetes\n",
    "    - Strong understanding of statistics\n",
    "    - Git version control\n",
    "    - Excellent problem-solving skills\n",
    "    \n",
    "    PREFERRED:\n",
    "    - Experience with Spark, Kafka, or Airflow\n",
    "    - NLP or computer vision experience\n",
    "    - Master's or Ph.D. in Computer Science\n",
    "    - Strong communication and leadership skills\n",
    "    \"\"\"\n",
    "\n",
    "# Process job description\n",
    "job_cleaned = clean_text(job_description)\n",
    "job_skills = extract_skills(job_description)\n",
    "\n",
    "print(f\"\\nJob: {job_title}\")\n",
    "print(f\"Required skills found: {len(job_skills)}\")\n",
    "print(f\"Skills: {sorted(job_skills)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Split resumes into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Split:\n",
      "  Training set: 1986 resumes (80%)\n",
      "  Testing set: 497 resumes (20%)\n",
      "\n",
      "Will screen 497 candidates from test set\n"
     ]
    }
   ],
   "source": [
    "if resume_df is not None and len(resume_df) > 0:\n",
    "    # Split data\n",
    "    train_df, test_df = train_test_split(\n",
    "        resume_df, \n",
    "        test_size=CONFIG['test_size'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Dataset Split:\")\n",
    "    print(f\"  Training set: {len(train_df)} resumes ({(1-CONFIG['test_size'])*100:.0f}%)\")\n",
    "    print(f\"  Testing set: {len(test_df)} resumes ({CONFIG['test_size']*100:.0f}%)\")\n",
    "    \n",
    "    # We'll use the test set for ranking (simulates real screening)\n",
    "    working_df = test_df.copy()\n",
    "    print(f\"\\nWill screen {len(working_df)} candidates from test set\")\n",
    "else:\n",
    "    print(\"No data for train/test split!\")\n",
    "    working_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Similarity Scores Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating text similarity scores...\n",
      "Calculated text similarity for 497 resumes\n",
      "  Mean similarity: 0.1406\n",
      "  Max similarity: 0.3228\n",
      "  Min similarity: 0.0298\n"
     ]
    }
   ],
   "source": [
    "if working_df is not None and len(working_df) > 0:\n",
    "    print(\"Calculating text similarity scores...\")\n",
    "    \n",
    "    # Prepare texts\n",
    "    all_texts = [job_cleaned] + working_df['cleaned_text'].tolist()\n",
    "    \n",
    "    # TF-IDF Vectorization\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=1000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        min_df=1\n",
    "    )\n",
    "    \n",
    "    tfidf_matrix = tfidf.fit_transform(all_texts)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:]).flatten()\n",
    "    \n",
    "    working_df['text_similarity'] = similarities\n",
    "    \n",
    "    print(f\"Calculated text similarity for {len(working_df)} resumes\")\n",
    "    print(f\"  Mean similarity: {similarities.mean():.4f}\")\n",
    "    print(f\"  Max similarity: {similarities.max():.4f}\")\n",
    "    print(f\"  Min similarity: {similarities.min():.4f}\")\n",
    "else:\n",
    "    print(\" No data for similarity calculation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Skill Match Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating skill match scores...\n",
      " Calculated skill matches for 497 resumes\n",
      "  Mean skill match: 23.21%\n",
      "  Best skill match: 72.73%\n"
     ]
    }
   ],
   "source": [
    "if working_df is not None and len(working_df) > 0:\n",
    "    print(\"Calculating skill match scores...\")\n",
    "    \n",
    "    def calculate_skill_match(resume_skills):\n",
    "        \"\"\"\n",
    "        Calculate what % of required skills the candidate has.\n",
    "        \"\"\"\n",
    "        if len(job_skills) == 0:\n",
    "            return 0.0, set(), job_skills\n",
    "        \n",
    "        matching = resume_skills.intersection(job_skills)\n",
    "        missing = job_skills - resume_skills\n",
    "        score = len(matching) / len(job_skills)\n",
    "        \n",
    "        return score, matching, missing\n",
    "    \n",
    "    # Apply to all resumes\n",
    "    results = working_df['extracted_skills'].apply(calculate_skill_match)\n",
    "    \n",
    "    working_df['skill_match_score'] = results.apply(lambda x: x[0])\n",
    "    working_df['matching_skills'] = results.apply(lambda x: x[1])\n",
    "    working_df['missing_skills'] = results.apply(lambda x: x[2])\n",
    "    working_df['num_matching'] = working_df['matching_skills'].apply(len)\n",
    "    working_df['num_missing'] = working_df['missing_skills'].apply(len)\n",
    "    \n",
    "    print(f\" Calculated skill matches for {len(working_df)} resumes\")\n",
    "    print(f\"  Mean skill match: {working_df['skill_match_score'].mean():.2%}\")\n",
    "    print(f\"  Best skill match: {working_df['skill_match_score'].max():.2%}\")\n",
    "else:\n",
    "    print(\"No data for skill matching!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Final Weighted Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating final scores...\n",
      "\n",
      "Scoring Formula:\n",
      "  Final Score = (30% × Text Similarity) + (70% × Skill Match)\n",
      "\n",
      "Score Statistics:\n",
      "  Mean: 20.47%\n",
      "  Std Dev: 10.93%\n",
      "  Max: 58.08%\n",
      "  Min: 0.90%\n"
     ]
    }
   ],
   "source": [
    "if working_df is not None and len(working_df) > 0:\n",
    "    print(\"Calculating final scores...\")\n",
    "    \n",
    "    # Weighted combination\n",
    "    working_df['final_score'] = (\n",
    "        CONFIG['text_similarity_weight'] * working_df['text_similarity'] +\n",
    "        CONFIG['skill_match_weight'] * working_df['skill_match_score']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nScoring Formula:\")\n",
    "    print(f\"  Final Score = ({CONFIG['text_similarity_weight']:.0%} × Text Similarity) + \"\n",
    "          f\"({CONFIG['skill_match_weight']:.0%} × Skill Match)\")\n",
    "    \n",
    "    print(f\"\\nScore Statistics:\")\n",
    "    print(f\"  Mean: {working_df['final_score'].mean():.2%}\")\n",
    "    print(f\"  Std Dev: {working_df['final_score'].std():.2%}\")\n",
    "    print(f\"  Max: {working_df['final_score'].max():.2%}\")\n",
    "    print(f\"  Min: {working_df['final_score'].min():.2%}\")\n",
    "else:\n",
    "    print(\" No data for final scoring!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "                                    CANDIDATE RANKINGS                                    \n",
      "==========================================================================================\n",
      "\n",
      "Top 10 Candidates:\n",
      "\n",
      "Rank   Candidate                 Final        Text Sim     Skill Match  Skills\n",
      "------------------------------------------------------------------------------------------\n",
      "1      Candidate_2078            58.08%       23.90%       72.73%       8/11\n",
      "2      Candidate_247             56.79%       19.61%       72.73%       8/11\n",
      "3      Candidate_893             50.19%       18.80%       63.64%       7/11\n",
      "4      Candidate_1266            48.55%       13.34%       63.64%       7/11\n",
      "5      Candidate_1212            45.38%       24.00%       54.55%       6/11\n",
      "6      Candidate_2451            45.26%       23.58%       54.55%       6/11\n",
      "7      Candidate_2331            44.16%       19.94%       54.55%       6/11\n",
      "8      Candidate_211             43.68%       18.32%       54.55%       6/11\n",
      "9      Candidate_255             43.66%       18.25%       54.55%       6/11\n",
      "10     Candidate_1928            43.63%       18.16%       54.55%       6/11\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "if working_df is not None and len(working_df) > 0:\n",
    "    # Sort by final score\n",
    "    ranked_df = working_df.sort_values('final_score', ascending=False).reset_index(drop=True)\n",
    "    ranked_df['rank'] = range(1, len(ranked_df) + 1)\n",
    "    \n",
    "    print(\"=\"*90)\n",
    "    print(\"CANDIDATE RANKINGS\".center(90))\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # Display top candidates\n",
    "    top_n = min(CONFIG['top_n_candidates'], len(ranked_df))\n",
    "    \n",
    "    print(f\"\\nTop {top_n} Candidates:\\n\")\n",
    "    print(f\"{'Rank':<6} {'Candidate':<25} {'Final':<12} {'Text Sim':<12} {'Skill Match':<12} {'Skills'}\")\n",
    "    print(\"-\"*90)\n",
    "    \n",
    "    for idx, row in ranked_df.head(top_n).iterrows():\n",
    "        candidate_name = str(row['candidate_name'])[:24]\n",
    "        print(f\"{row['rank']:<6} {candidate_name:<25} \"\n",
    "              f\"{row['final_score']:.2%}{'':6} \"\n",
    "              f\"{row['text_similarity']:.2%}{'':6} \"\n",
    "              f\"{row['skill_match_score']:.2%}{'':6} \"\n",
    "              f\"{row['num_matching']}/{len(job_skills)}\")\n",
    "    \n",
    "    print(\"=\"*90)\n",
    "else:\n",
    "    print(\"No data to rank!\")\n",
    "    ranked_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Analysis of Top Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "                               DETAILED CANDIDATE ANALYSIS                                \n",
      "==========================================================================================\n",
      "\n",
      "RANK #1: Candidate_2078\n",
      "------------------------------------------------------------------------------------------\n",
      "Category: PUBLIC-RELATIONS\n",
      "\n",
      "Scores:\n",
      "  Final Score: 58.08%\n",
      "  ├─ Text Similarity: 23.90%\n",
      "  └─ Skill Match: 72.73% (8/11 skills)\n",
      "\n",
      "Total Skills Identified: 14\n",
      "\n",
      "✓ MATCHING SKILLS (8):\n",
      "  communication, excel, leadership, microsoft office, powerpoint\n",
      "  project management, r, word\n",
      "\n",
      "MISSING SKILLS (3):\n",
      "  agile, collaboration, innovative\n",
      "\n",
      "\n",
      "RANK #2: Candidate_247\n",
      "------------------------------------------------------------------------------------------\n",
      "Category: INFORMATION-TECHNOLOGY\n",
      "\n",
      "Scores:\n",
      "  Final Score: 56.79%\n",
      "  ├─ Text Similarity: 19.61%\n",
      "  └─ Skill Match: 72.73% (8/11 skills)\n",
      "\n",
      "Total Skills Identified: 11\n",
      "\n",
      "✓ MATCHING SKILLS (8):\n",
      "  collaboration, communication, excel, leadership, microsoft office\n",
      "  powerpoint, project management, word\n",
      "\n",
      "MISSING SKILLS (3):\n",
      "  agile, innovative, r\n",
      "\n",
      "\n",
      "RANK #3: Candidate_893\n",
      "------------------------------------------------------------------------------------------\n",
      "Category: FITNESS\n",
      "\n",
      "Scores:\n",
      "  Final Score: 50.19%\n",
      "  ├─ Text Similarity: 18.80%\n",
      "  └─ Skill Match: 63.64% (7/11 skills)\n",
      "\n",
      "Total Skills Identified: 9\n",
      "\n",
      "✓ MATCHING SKILLS (7):\n",
      "  agile, collaboration, communication, innovative, leadership\n",
      "  microsoft office, project management\n",
      "\n",
      "MISSING SKILLS (4):\n",
      "  excel, powerpoint, r, word\n",
      "\n",
      "\n",
      "RANK #4: Candidate_1266\n",
      "------------------------------------------------------------------------------------------\n",
      "Category: DIGITAL-MEDIA\n",
      "\n",
      "Scores:\n",
      "  Final Score: 48.55%\n",
      "  ├─ Text Similarity: 13.34%\n",
      "  └─ Skill Match: 63.64% (7/11 skills)\n",
      "\n",
      "Total Skills Identified: 15\n",
      "\n",
      "✓ MATCHING SKILLS (7):\n",
      "  communication, excel, innovative, leadership, powerpoint\n",
      "  project management, word\n",
      "\n",
      "MISSING SKILLS (4):\n",
      "  agile, collaboration, microsoft office, r\n",
      "\n",
      "\n",
      "RANK #5: Candidate_1212\n",
      "------------------------------------------------------------------------------------------\n",
      "Category: CONSULTANT\n",
      "\n",
      "Scores:\n",
      "  Final Score: 45.38%\n",
      "  ├─ Text Similarity: 24.00%\n",
      "  └─ Skill Match: 54.55% (6/11 skills)\n",
      "\n",
      "Total Skills Identified: 12\n",
      "\n",
      "✓ MATCHING SKILLS (6):\n",
      "  excel, leadership, microsoft office, powerpoint, project management\n",
      "  word\n",
      "\n",
      "MISSING SKILLS (5):\n",
      "  agile, collaboration, communication, innovative, r\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "if ranked_df is not None and len(ranked_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"DETAILED CANDIDATE ANALYSIS\".center(90))\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    # Show detailed analysis for top 5\n",
    "    detailed_top_n = min(5, len(ranked_df))\n",
    "    \n",
    "    for idx, row in ranked_df.head(detailed_top_n).iterrows():\n",
    "        print(f\"\\nRANK #{row['rank']}: {row['candidate_name']}\")\n",
    "        print(\"-\"*90)\n",
    "        \n",
    "        if resume_category_col and resume_category_col in row:\n",
    "            print(f\"Category: {row[resume_category_col]}\")\n",
    "        \n",
    "        print(f\"\\nScores:\")\n",
    "        print(f\"  Final Score: {row['final_score']:.2%}\")\n",
    "        print(f\"  ├─ Text Similarity: {row['text_similarity']:.2%}\")\n",
    "        print(f\"  └─ Skill Match: {row['skill_match_score']:.2%} ({row['num_matching']}/{len(job_skills)} skills)\")\n",
    "        \n",
    "        print(f\"\\nTotal Skills Identified: {row['num_skills']}\")\n",
    "        \n",
    "        # Matching skills\n",
    "        if row['num_matching'] > 0:\n",
    "            print(f\"\\nMATCHING SKILLS ({row['num_matching']}):\")\n",
    "            matching_list = sorted(row['matching_skills'])\n",
    "            for i in range(0, len(matching_list), 5):\n",
    "                print(\"  \" + \", \".join(matching_list[i:i+5]))\n",
    "        else:\n",
    "            print(f\"\\nMATCHING SKILLS: None\")\n",
    "        \n",
    "        # Missing skills\n",
    "        if row['num_missing'] > 0:\n",
    "            print(f\"\\nMISSING SKILLS ({row['num_missing']}):\")\n",
    "            missing_list = sorted(row['missing_skills'])\n",
    "            for i in range(0, len(missing_list), 5):\n",
    "                print(\"  \" + \", \".join(missing_list[i:i+5]))\n",
    "        else:\n",
    "            print(f\"\\nMISSING SKILLS: None (Has all required skills!)\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    print(\"=\"*90)\n",
    "else:\n",
    "    print(\"No data for detailed analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================================================================\n",
      "                                    SCREENING SUMMARY                                     \n",
      "==========================================================================================\n",
      "\n",
      "Job Position: Johnson & Johnson Family of Companies Job Application for Senior Training Leader | Monster.com var MONS_LOG_VARS = {\"JobID\":\n",
      "Total Candidates Screened: 497\n",
      "Required Skills: 11\n",
      "\n",
      "Score Distribution:\n",
      "  Excellent (≥70%): 0 candidates (0.0%)\n",
      "  Good (50-70%): 3 candidates (0.6%)\n",
      "  Fair (30-50%): 99 candidates (19.9%)\n",
      "  Poor (<30%): 395 candidates (79.5%)\n",
      "\n",
      "TOP CANDIDATE:\n",
      "  Name: Candidate_2078\n",
      "  Score: 58.08%\n",
      "  Skills Matched: 8/11\n",
      "  Recommendation: Good fit - Recommended for interview\n",
      "\n",
      "Most Common Skills Across Candidates:\n",
      "  communication: 285 candidates (57.3%)\n",
      "  leadership: 196 candidates (39.4%)\n",
      "  excel: 179 candidates (36.0%)\n",
      "  word: 142 candidates (28.6%)\n",
      "  microsoft office: 139 candidates (28.0%)\n",
      "  testing: 104 candidates (20.9%)\n",
      "  project management: 94 candidates (18.9%)\n",
      "  creative: 91 candidates (18.3%)\n",
      "  powerpoint: 89 candidates (17.9%)\n",
      "  problem solving: 84 candidates (16.9%)\n",
      "\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "if ranked_df is not None and len(ranked_df) > 0:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(\"SCREENING SUMMARY\".center(90))\n",
    "    print(\"=\"*90)\n",
    "    \n",
    "    print(f\"\\nJob Position: {job_title}\")\n",
    "    print(f\"Total Candidates Screened: {len(ranked_df)}\")\n",
    "    print(f\"Required Skills: {len(job_skills)}\")\n",
    "    \n",
    "    # Score distribution\n",
    "    print(f\"\\nScore Distribution:\")\n",
    "    excellent = len(ranked_df[ranked_df['final_score'] >= 0.7])\n",
    "    good = len(ranked_df[(ranked_df['final_score'] >= 0.5) & (ranked_df['final_score'] < 0.7)])\n",
    "    fair = len(ranked_df[(ranked_df['final_score'] >= 0.3) & (ranked_df['final_score'] < 0.5)])\n",
    "    poor = len(ranked_df[ranked_df['final_score'] < 0.3])\n",
    "    \n",
    "    print(f\"  Excellent (≥70%): {excellent} candidates ({excellent/len(ranked_df)*100:.1f}%)\")\n",
    "    print(f\"  Good (50-70%): {good} candidates ({good/len(ranked_df)*100:.1f}%)\")\n",
    "    print(f\"  Fair (30-50%): {fair} candidates ({fair/len(ranked_df)*100:.1f}%)\")\n",
    "    print(f\"  Poor (<30%): {poor} candidates ({poor/len(ranked_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Top candidate\n",
    "    top = ranked_df.iloc[0]\n",
    "    print(f\"\\nTOP CANDIDATE:\")\n",
    "    print(f\"  Name: {top['candidate_name']}\")\n",
    "    print(f\"  Score: {top['final_score']:.2%}\")\n",
    "    print(f\"  Skills Matched: {top['num_matching']}/{len(job_skills)}\")\n",
    "    \n",
    "    # Recommendation\n",
    "    if top['final_score'] >= 0.7:\n",
    "        recommendation = \"Strong fit - Highly recommended for interview\"\n",
    "    elif top['final_score'] >= 0.5:\n",
    "        recommendation = \"Good fit - Recommended for interview\"\n",
    "    elif top['final_score'] >= 0.3:\n",
    "        recommendation = \"Moderate fit - Consider based on other factors\"\n",
    "    else:\n",
    "        recommendation = \"Weak fit - May not be suitable for this role\"\n",
    "    \n",
    "    print(f\"  Recommendation: {recommendation}\")\n",
    "    \n",
    "    # Most common skills across all candidates\n",
    "    all_skills_flat = []\n",
    "    for skills in ranked_df['extracted_skills']:\n",
    "        all_skills_flat.extend(skills)\n",
    "    \n",
    "    from collections import Counter\n",
    "    skill_counts = Counter(all_skills_flat)\n",
    "    \n",
    "    print(f\"\\nMost Common Skills Across Candidates:\")\n",
    "    for skill, count in skill_counts.most_common(10):\n",
    "        percentage = count / len(ranked_df) * 100\n",
    "        print(f\"  {skill}: {count} candidates ({percentage:.1f}%)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "else:\n",
    "    print(\"No data for summary!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results exported to: resume_screening_results.csv\n",
      "  Rows: 497\n",
      "  Columns: 14\n",
      "\n",
      "Preview of exported data:\n",
      "   rank  candidate_name  final_score_%  num_matching\n",
      "0     1  Candidate_2078          58.08             8\n",
      "1     2   Candidate_247          56.79             8\n",
      "2     3   Candidate_893          50.19             7\n",
      "3     4  Candidate_1266          48.55             7\n",
      "4     5  Candidate_1212          45.38             6\n",
      "5     6  Candidate_2451          45.26             6\n",
      "6     7  Candidate_2331          44.16             6\n",
      "7     8   Candidate_211          43.68             6\n",
      "8     9   Candidate_255          43.66             6\n",
      "9    10  Candidate_1928          43.63             6\n"
     ]
    }
   ],
   "source": [
    "if ranked_df is not None and len(ranked_df) > 0:\n",
    "    # Prepare export DataFrame\n",
    "    export_df = ranked_df[[\n",
    "        'rank', 'candidate_name', 'final_score', 'text_similarity', \n",
    "        'skill_match_score', 'num_skills', 'num_matching', 'num_missing'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Add category if available\n",
    "    if resume_category_col and resume_category_col in ranked_df.columns:\n",
    "        export_df['category'] = ranked_df[resume_category_col]\n",
    "    \n",
    "    # Convert skills to string for CSV\n",
    "    export_df['matching_skills'] = ranked_df['matching_skills'].apply(lambda x: ', '.join(sorted(x)))\n",
    "    export_df['missing_skills'] = ranked_df['missing_skills'].apply(lambda x: ', '.join(sorted(x)))\n",
    "    \n",
    "    # Format percentages\n",
    "    export_df['final_score_%'] = (export_df['final_score'] * 100).round(2)\n",
    "    export_df['text_similarity_%'] = (export_df['text_similarity'] * 100).round(2)\n",
    "    export_df['skill_match_%'] = (export_df['skill_match_score'] * 100).round(2)\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_file = 'resume_screening_results.csv'\n",
    "    export_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"Results exported to: {output_file}\")\n",
    "    print(f\"  Rows: {len(export_df)}\")\n",
    "    print(f\"  Columns: {len(export_df.columns)}\")\n",
    "    \n",
    "    # Show preview\n",
    "    print(f\"\\nPreview of exported data:\")\n",
    "    print(export_df[['rank', 'candidate_name', 'final_score_%', 'num_matching']].head(10))\n",
    "else:\n",
    "    print(\" No data to export!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
